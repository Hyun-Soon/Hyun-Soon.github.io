---
title: \[DeepLearning\] Neural Network 및 Perceptron
excerpt:
categories:
tags:
---
# 0. Neural Network란?

신경망(Neural Network)란 인간의 뇌가 작동하는 방식을 본떠 만든 컴퓨팅 시스템으로, 상호 연결된 노드(뉴런)들이 데이터를 처리하여 패턴을 인식하고, 학습하며, 미래를 예측하는 인공지능(AI) 모델의 핵심이다.

## 0-1. Neural Network의 주요 구성 요소

- 노드(뉴런) : 뇌의 뉴런처럼 데이터를 받아 처리하는 처리 단위

- 계층
	신경망은 여러 층으로 이루어져 있다.
	- `입력층` : 외부 데이터를 처음으로 받아들이는 층
	- `은닉층` : 입력과 출력 사이에서 데이터를 처리하고 복잡한 특징을 학습하는 층
	- `출력층` : 최정적인 처리 결과를 내놓는 층

- 가중치 및 편향 : 노드 간 연결 강도를 나타내는 수치로, 신경망이 학습하면서 조정된다.

## 0-2. Neural Network의 작동 방식

1. `데이터 입력` : 입력층을 통해 데이터가 신경망으로 들어온다.
2. `데이터 처리` : 입력 데이터는 연결된 노드들을 거치면서 가중치와 편향이 적용된 수학적 연산을 수행한다.
3. `활성화 및 전달` : 노드는 특정 임계값(Threshold)을 넘는 활성화 신호가 발생하면 다음 계층으로 데이터를 전달한다.
4. `학습 및 예측` : 신경망은 충분한 학습 데이터를 통해 데이터의 패턴을 인식하고 분류하며, 이를 바탕으로 새로운 데이터에 대한 예측을 할 수 있다.

## 0-3. 주의할 점

특정 데이터에만 집중시켜 학습하면 `overfitting` 문제가 발생할 수 있음.

`overfitting` : 신경망의 학습이 특정 데이터로 집중적으로 이루어져, 새로운 케이스(해결해야 하는 상황의 데이터)에 대해 유효한 결과를 내놓지 못하는 문제

# 1. Perceptron

`Perceptron`은 신경망(딥러닝)의 기원이 되는 알고리즘이다. 퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 여기서 말하는 `신호`란 전류나 강물처럼 흐름이 있는 것을 상상하면 좋다. 퍼셉트론의 신호는 `흐른다/안흐른다(1 또는 0)`의 값을 가진다.

![[Pasted image 20250902151102.png]]

위는 입력으로 2개의 신호를 받은 퍼셉트론의 그림이다. x1과 x2는 `입력 신호`, w1과 w2는 `가중치`, y는 `출력 신호`를 의미한다. 뉴런에서 보내온 신호의 총합이 정해진 한계를 넘어설 때만 1을 출력(활성화)한다. 이 한계를 `임계값(θ)`이라고 한다.

![[Pasted image 20250902151521.png]]

퍼셉트론은 복수의 입력 신호 각각에 고유한 가중치를 부여한다. 가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용한다. 즉, 가중치가 클수록 해당 신호가 그만큼 더 중요함을 의미한다.

## 1-1. Perceptron의 한계

하나의 Perceptron으로는 XOR Gate를 만들 수 없다.


**XOR Gate 진리표**

| x1  | x2  |  y  |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  0  |  1  |  1  |
|  1  |  0  |  1  |
|  1  |  1  |  1  |
하나의 Perceptron은 하나의 임계값을 기준으로 두 구역을 나눈다. x1, x2를 축으로 하는 좌표상의 한 직선을 생각했을 때, XOR의 결과값은 하나의 직선으로 나눌 수 없도록 분포되어 있다. 즉, 선형 영역으로는 불가능하고 비선형 영역으로 구분해야 한다.

![[Pasted image 20250902165328.png]]

이처럼 선형 분류로 불가능한 문제를 해결하기 위해 `MLP(Multi-Layer Perceptron)` 개념이 등장하게 된다.

# 2. MLP(Multi-Layer Perceptron)

MLP란 지도학습에 사용되는 인공 신경망의 한 형태이다. MLP는 일반적으로 최소 하나 이상의 비선형 은닉 계층을 포함하며, 이러한 계층은 학습 데이터에서 복잡한 패턴을 추출하는 데 도움이 된다. MLP는 주로 `분류` 및 `회귀` 문제에 적용되며, 그 학습 알고리즘으로 역전파가 주로 사용된다.

MLP의 각 노드는 다음 계층의 모든 노드와 연결되어 있다(Fully-connected). 즉, MLP의 모든 노드는 이전 계층 정보를 모두 활용한다.

**특징**
- Linear Stacking of Layers : Layer가 정해진 순서대로 추가된다.
- Single Input, Single Output(typically) : Sequential model은 주된 하나의 인풋, 아웃풋이 있을 때 가장 효과적이다.
- Simplicity and Ease of Use : Keras와 같은 framework의 sequential API는 직선적인 방식의 네트워크를 정의한다. 이는 통상적인 딥러닝 태스크에 이상적이다.
- Dense Layers : MLP는 주로 "dense"하거나 "fully connected"된 레이어를 사용한다. 즉, 모든(대부분의) 뉴런이 다음 레이어의 모든 뉴런과 연결된다.
- Activation Function : 네트워크가 복잡한 패턴을 학습할 수 있도록 일반적으로 Non-linear 활성함수가 사용된다.

## 2-1. Loss Function

손실 함수는 모델이 예측한 값과 실제 값 간의 차이를 계산하여 모델의 성능을 평가한다. 손실 함수의 값이 작을 수록 모델의 예측이 실제 값에 가깝다는 것을 의미한다.

딥러닝 모델의 목표는 손실 함수의 값을 최소화하는 것이다. 모델은 학습 과정에서 가중치를 조정하며 손실 함수의 값을 줄여 나간다.

손실 함수의 선택은 문제 유형과 데이터 특성에 따라 달라진다. 적절한 손실 함수를 선택함으로써 모델의 성능을 최적화하고, 학습 과정에서 더 나은 결과를 도출할 수 있다.

그런데 왜 `정확도`라는 직관적인 지표를 놔두고, `손실 함수의 값`이라는 우회적인 방법을 택하는 것일까? 이 의문을 해결하기 위해선 신경망 학습에서의 `미분`의 역할에 주목해야 한다. 신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 만드는 매개변수 값을 찾는다. 이때 매개변수의 미분(정확히는 기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다. 

정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다. 예시를 보자. 한 신경망이 100장의 훈련 데이터 중 32장을 올바로 인식한다고 하면 정확도는 32%이다. 매개변수를 약간만 조정해도 정확도는 개선되지 않고 일정하게 유지된다. 정확도가 개선된다 하더라도 그 값은 32.0131%와 같은 연속적인 변화보다는 33%나 34%처럼 불연속적인 띄엄띄엄한 값으로 바뀌어버린다. 반면 손실 함수를 지표로 삼는다면, 현재의 손실 함수의 값은 0.92543..., 매개변수 값이 조금 바뀌면 0.93432...처럼 연속적으로 변화한다.

손실 함수는 일반적으로 `오차제곱합`과 `교차 엔트로피 오차`를 사용한다.

### 2-1-1. SSE(Sum of Squares for Error, 오차제곱합)

![[Pasted image 20250903155700.png]]

`y_k` : 신경망의 출력(신경망이 추정한 값)
`t_k` : 정답 레이블
`k` : 데이터의 차원 수


### 2-1-2. CEE(Cross Entropy Error, 교차 엔트로피 오차)

![[Pasted image 20250903160117.png]]

`y_k` : 신경망의 출력
`t_k` : 정답 레이블

t_k는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다(`원-핫 인코딩`).
즉, 시그마가 있지만 실제 오차값은 정답에 해당하는 노드만 영향을 준다.

!! 그런데, `y=log(x)`의 그래프를 생각해보자. x가 0에 가까울 수록 y값은 -inf로 간다. 이 경우, 언더플로우가 나거나 계산 진행이 불가능해진다. 따라서 아주 작은 값을 더해서 절대 0이 되지 않도록 방지를 해야 한다.

![[Pasted image 20250903161134.png]]

위의 식을 N개의 데이터로 이루어진 `batch`로 확장하면 아래와 같다.

![[Pasted image 20250903161857.png]]

`t_nk`는 n번째 데이터의 k번째 값을 의미한다.
`y_nk`는 신경망의 출력을 의미한다.

### 2-1-3. MSE(Mean Squared Error, 평균 제곱 오차)

주로 회귀 문제에서 사용된다. 예측 값과 실제 값 사이의 차이를 제곱한 후, 그 값들의 평균을 구하는 방식이다. 

MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2

y_i는 실제 값을,
\hat{y}i는 예측 값을,
n은 데이터의 총 개수를 의미한다.

MSE는 오차가 클수록 그 값이 제곱되므로, 더 큰 패널티를 부여한다. 이는 이상치(outliner)에 민감하다는 단점이 있을 수 있지만, MSE를 통해 모델이 큰 오차를 더 중요하게 다루도록 유도할 수 있다.


## 2-2. MLP의 방식

- `epoch` : 모든 데이터 셋을 한 번 학습
- `batch` : 방대한 데이터를 작게 나눈 단위. 각 batch의 크기를 `batch size`라고 한다.
- `iteration` : 1 epoch를 마치는데 필요한 반복 횟수. `data size` / `batch size`

### 2-2-1. Sequential 방식

학습 루프를 한 번 돌 때마다, 에러율을 확인하고 epoch를 반복한다.

### 2-2-2. Batch 방식

batch는 나눠진 데이터 셋을 의미한다. 데이터가 방대한 경우, 메모리 한계와 속도 저하 때문에 대부분의 경우 한 번의 epoch에 모든 데이터를 집어 넣을 수 없다. 따라서 데이터를 batch로 나누고, 각 batch를 iteration 마다 학습시킨다.

# 3. Activation Function

입력 신호의 총합을 출력 신호로 변환하는 함수이다.

```text
a = b + w_1 * x_1 + w_2 * x_2
y = h(a) // 활성 함수
```

활성 함수로는 비선형 함수를 사용한다. 선형 함수를 사용하면 여러 개의 은닉층을 사용하는 의미가 없다. 아래 예시를 보자.

```text
h(x) = cx // 활성 함수
y(x) = h(h(h(x))) = c * c * c * x = c^3 * x

let a = c^3
y(x) = ax

즉, 선형 함수를 활성 함수로 사용하면 은닉층이 없는 네트워크로 표현할 수 있다.
은닉층이 의미가 없다.
```



## 3-1. Sigmoid 함수 

![[Pasted image 20250903105055.png]]
![[Pasted image 20250903105816.png]]

## 3-2. ReLU 함수

![[Pasted image 20250903105857.png]]

![[Pasted image 20250903110037.png]]

## 3-3. 출력층의 활성화 함수

### 3-3-1. 항등 함수

입력과 출력이 같은 함수. 보통 회귀(Regression)에 많이 사용된다.

왜 이름이 회귀일까? : 19세기 후반 영국의 우생학자 프랜시스 골턴 경은 사람과 완두콩 등을 대상으로 그 키(크기)를 측정했다. 관찰 결과 키가 큰 부모의 자식은 부모보다 작고 작은 부모의 자식은 부모보다 큰, 즉 평균으로 회귀하는 경향이 있음을 알아냈다. 그 사이에는 선형 관계가 있어 부모의 키로부터 자식의 키를 예측할 수 있고, 그 예측 결괏값이 연속적인 수치인 것이다. 따라서 입력 데이터에서 (연속적인) 수치를 예측하는 문제를 회귀라고 부르게 되었다.

### 3-3-2. 소프트맥스(softmax) 함수

![[Pasted image 20250903134310.png]]

softmax 함수를 구현할 때는, 오버플로우 문제를 조심해야 한다. 지수함수는 급격히 증가한다.

아래는 오버플로우 문제를 개선한 수식이다.

![[Pasted image 20250903134642.png]]

보통 `C_prime`은 노드의 출력값 중 최댓값(max(a))을 활용한다.
만약 C_prime = -max(a)라고 하면, 배열 a안의 값은 0보다 큰값이 없게 되므로 오버플로우 문제를 막을 수 있다.

소프트맥스를 활성화 함수로 한 출력 노드들의 결과값의 합은 1이다.
따라서, 소프트 맥스 함수의 출력을 `확률`로 해석할 수 있다.

소프트맥스 함수는 단조증가 함수이다. 즉, `a<b`이면 `softmax(a) < softmax(b)`, 인자의 대소관계가 변하지 않는다. 신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식하기 때문에, 함수를 적용해도 출력이 가장 큰 뉴런의 위치가 달라지지 않는 소프트맥스는 생략이 가능하다. 현업에서도 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는 것이 일반적이다.

!! 한편, 신경망을 학습시킬 때는 출력층에서 소프트맥스 함수를 사용한다.


# 4. BackPropagation(오차역전파)

`weight`, `bias`의 값을 조정하기 위해 수치 미분 방식으로 기울기를 계산한다. `오차역전파`는 기울기 계산을 고속으로 수행하는 기법이다.

1. 덧셈 노드의 역전파

![[Pasted image 20250905164100.png]]

2. 곱셈 노드의 역전파

![[Pasted image 20250905164126.png]]


https://wikidocs.net/227541

손실함수 -> 신경망을 거쳐서 나온 결과값의 오차를 구하는 함수 -> w의 변화에 따라 값이 달라짐
L(x, w, b)
-> x의 종류에 따라 원하는 답이 달라진다.
-> x의 종류에 따라 w, b를 찾아야 한다.


- sequential 방식 vs batch 방식
나는 sequential 방식이, 모든 데이터 한번 학습한 후 backpropagation 한번, 이걸 epoch만큼 반복하는 것이 sequential 방식인 줄 알았는데, 그게 아니고 데이터 하나 학습 당 backpropagation 한번이었다. 모든 데이터 한번 학습한 후 backpropagation 한번 하는 건 그냥 batch 사이즈가 전체 데이터와 같은 batch 방식.

# 5. Classification(분류)과 Regression(회귀)

## 5-1. Regression

학습 시간이 시험 성적에 미치는 영향, 수면의 질이 건강에 미치는 영향 등 어떠한 변수들로부터 결과를 예측하는 작업이다. 영향을 미치는 변수를 `독립변수`, 영향을 받는 변수를 `종속변수`라고 한다. 즉 위의 예시에서 독립변수는 학습 시간과 수면의 질이고, 종속변수는 시험 성적과 건강이다.

### 5-1-1. Regression 평가 지표

회귀 모델을 훈련해 최적의 회귀계수를 구할 수 있다. 최적 회귀계수를 구하려면 예측값과 실젯값의 차이, 즉 오차를 최소화해야 한다(Overfitting 조심).

![[Pasted image 20250912142504.png]]

### 5-1-2. Correlation(상관계수)

두 변수 사이의 상관관계 정도를 수치로 나타낸 값을 `상관계수`라고 한다. 상관계쑤는 보통 약자 `r`로 표기한다. 여러 상관계수가 있지만 가장 많이 쓰는 선형 회귀 상관계수는 `피어슨 상관계수`이다. 피어슨 상관계수는 선형 상관관계의 강도와 방향을 나타내며, -1 ~ 1 사이의 값을 갖는다.

![[Pasted image 20250912143136.png]]

## 5-2. Classification

`분류`란 어떤 대상을 정해진 범주에 구분해 넣는 작업이다. `Feature(특징)`을 토대로 `Target`을 판별한다. Target값이 2개인 분류를 `이진분류`, 3개 이상인 분류를 `다중분류`라고 한다.

### 5-2-1. 분류 평가지표

분류 문제에서 사용되는 기본적인 지표로 `오차 행렬`, `로그 손실`, `ROC 곡선`, `AUC`가 있다.

#### 5-2-1-1. Confusion Matrix(오차 행렬)

![[Pasted image 20250912150014.png]]

 실제 타깃값과 예측한 타깃값이 어떻게 매칭되는지를 보여주는 표이다. `혼동 행렬`이라고도 한다. 참 양성, 거짓 양성, 거짓 음성, 참 음성은 각각 약어로 `TP`, `FP`, `FN`, `TN`으로 표시한다. 오차 행렬을 활용한 주요 평가지표로는 정확도, 정밀도, 재현율, F1 점수가 있으며, 모두 값이 클수록 좋은 지표이다.

- `정확도`

![[Pasted image 20250912144111.png]]

정확도를 평가지표로 사용하는 경우는 많지 않다. 10일 중 1일 꼴로 비가 온다고 가정해보자. 매일 비가 '안'온다고 예측해도 정확도는 90%이다.

- `정밀도`

![[Pasted image 20250912144546.png]]

정밀도는 음성을 양성으로 잘못 판단하면 문제가 발생하는 경우에 사용한다. 스팸 메일 필터링을 예로 생각해 보자. 스팸 메일은 양성, 일반 메일은 음성이다. 음성을 양성으로 잘못 판단하면, 즉 일반 메일을 스팸으로 잘못 판단하면 업무상 중요한 메일인데 받지 못할 수 있다. 결론적으로 스팸 필터링은 음성을 양성으로 잘못 판단하는 경우에 더 문제가 된다. 이럴 때 정밀도를 사용할 수 있다.

- `재현율`

![[Pasted image 20250912145722.png]]

재현율은 양성을 음성으로 잘못 판단하면 문제가 되는 경우에 사용한다. 암인데(양성인데) 암이 아니라고(음성이라고) 진단하면 문제가 발생한다.

- `F1 점수`

![[Pasted image 20250912150118.png]]

F1 점수는 정밀도와 재현율을 조합한 평가 지표이다. 정밀도와 재현율 중 어느 하나에 편중하지 않고 적절히 조합하고 싶을 때 사용한다. F1 점수는 정밀도와 재현율의 조화 평균으로 구한다.


#### 5-2-1-2. LogLoss(로그 손실)

![[Pasted image 20250912151412.png]]

`y` : 실제 타깃값
`yHat` : 타깃값일 예측 확률

로그 손실은 분류 문제에서 타깃값을 확률로 예측할 때 기본적으로 사용하는 평가지표이다. 값이 **작을수록** 좋은 지표이다(이름에서 알 수 있듯이, 손실이 작을수록 좋다).

이진 분류 문제를 생각해보자. y는 0(음성) 또는 1(양성)이다. yHat은 타깃값이 1(양성)일 예측 확률이다(0 <= yHat <= 1).

![[Pasted image 20250912151842.png]]

위 그림의 경우, 세 값의 평균에 음의 부호를 취한 값인 0.2284가 최종 로그 손실값이다.

#### 5-2-1-3. ROC 곡선과 AUC

`ROC 곡선(Receiver Operating Characteristic`은 참 양성 비율(TPR)에 대한 거짓 양성 비율(FPR) 곡선이다. `AUC(Area Under the Curve`는 ROC 곡선 아래 면적을 말한다. AUC는 기본적으로 예측값이 확률인 분류 문제에서 사용한다.

![[Pasted image 20250912152550.png]]

# 6. 데이터 인코딩

머신러닝 모델은 문자 데이터를 인식하지 못한다. 그렇기 때문에 문자로 구성된 범주형 데이터는 숫자로 바꿔야 한다. 경우에 따라서 이미 숫자로 구성된 범주형 데이터도 모델 성능을 향상하기 위해 다른 숫자 데이터로 바꾸기도 한다. 이렇듯 범주형 데이터를 숫자 형태로 바꾸는 작업을 `데이터 인코딩`이라고 한다. 대표적인 인코딩 방식으로는 `레이블 인코딩`과 `원-핫 인코딩`이 있다.

## 6-1. 레이블 인코딩

레이블 인코딩은 범주형 데이터를 숫자로 일대일 매핑해주는 인코딩 방식이다. 레이블 인코딩은 간단하지만, 단점이 있다. 명목형 데이터를 레이블 인코딩하면 모델 성능이 떨어질 수 있다는 것이다. 머신러닝 모델이 서로 가까운 숫자를 비슷한 데이터라고 판단하기 때문이다. 머신러닝 모델은 1(바나나)과 3(사과)보다 1(바나나)과 2(블루베리)를 더 비슷한 데이터라고 인식한다. 하지만 실제로는 이들은 별개의 데이터일 뿐이다. 이 문제는 `원-핫 인코딩`으로 해결할 수 있다.

`명목형 데이터` : 고유의 순서가 없이 구분되는 데이터(ex. Red, Blue, Green, ...)

![[Pasted image 20250912152937.png]]

## 6-2. 원-핫 인코딩

원-핫 인코딩은 여러 값 중 하나만 활성화하는 인코딩이다. 실행 절차는 다음과 같다.

1. 인코딩하려는 피처의 고윳값 개수를 구한다.
2. 피처의 고윳값 개수만큼 열을 추가한다.
3. 각 고윳값에 해당하는 열에 1을 표시하고 나머지 열에는 0을 표시한다.

![[Pasted image 20250912154131.png]]
원-핫 인코딩은 레이블 인코딩의 문제(서로 가까운 숫자를 비슷한 데이터로 판단하는 문제)를 해결한다. 그렇지만 원-핫 인코딩도 열 개수가 지나치게 많아진다는 단점이 있다. 피처의 고윳값이 많으면 그만큼 열 개수와 메모리 사용량이 늘어나기 때문에 모델 훈련 속도가 느려질 우려가 있다.

***그렇다면 명목형 피처에 고윳값이 상당히 많을 땐 어떻게 해결해야 할까?***

1. 비슷한 고윳값끼리 그룹화 : 그룹화하면 해당 명목형 피처의 고윳값 개수가 줄어드는 효과가 있다.
2. 빈도가 낮은 고윳값을 '기타(etc)'로 처리하기 : 비슷한 고윳값끼리 그룹화하는 방법과 비슷하다. 빈도가 낮은 고윳값들을 묶어 '기타 고윳값'으로 일괄 처리하는 방법이다.
3. 다른 인코딩 적용하기 : 타깃 인코딩, 프리퀀시 인코딩 등 그 외 인코딩 기법이 있다.


# 7. Optimizer

딥러닝 학습시 최대한 틀리지 않는 방향으로 학습해야 한다.
얼마나 틀리는지(Loss)를 알게 하는 함수가 Loss Function이다. Loss Function의 최솟값을 찾는 것을 학습 목표로 한다.

`Optimization` : 최소값을 찾아가는 것
`Optimizer` : Optimization을 수행하는 알고리즘

## 7-1. Optimizer의 종류

![[Pasted image 20250912162008.png]]

![[Pasted image 20250912170111.png]]

- `Momentum` : 
- `Adam` : 

# 8. SGD(확률적 경사 하강법) vs BGD(배치 경사 하강법)

## 8-1. SGD(Stochastic Gradient Descent)

SGD(확률적 경사 하강법)는 머신러닝, 특히 딥러닝에서 사용되는 가장 대표적인 최적화 알고리듬이다. 전체 데이터셋을 사용하여 비용 함수의 기울기를 계산하는 BGD(배치 경사 하강법)와는 달리, SGD는 한번에 하나 또는 일부의 훈련 샘플을 사용하여 그래디언트를 계산하고 매개 변수를 업데이트한다. 이 방식은 계산 비용을 절감하며, 비선형 최적화 문제에 대한 솔루션을 빠르게 근사할 수 있다.

그러나 SGD는 수렴 속도가 느리고, 최적화 문제에 따라 최솟값을 찾는 데 어려움이 있을 수 있다. 이는 이동 단계마다 하나 또는 일부 샘플만 사용하기 때문에 그래디언트의 방향이 불안정하고 노이즈가 많을 수 있기 때문이다. 이 문제를 해결하기 위해 SGD의 변형이 많이 개발되었다. 예를 들어, `Momentum` 방법은 이전 그라디언트를 사용하여 현재 그라디언트를 보정하고, `AdaGrad`는 학습률을 동적으로 조정한다.

## 8-2. BGD(Batch Gradient Descent)

BGD(배치 경사 하강법)란 전체 훈련 세트를 사용하여 경사 하강법을 실행하는 최적화 알고리즘이다. 머신러닝 및 딥러닝에서 많이 사용되며, 주로 비용 함수를 최소화하기 위해 반복적으로 모델의 매개변수를 업데이트하는 데 사용된다.

BGD는 전체 데이터 세트에 대해 하나의 업데이트를 수행하는 데 비해 계산 비용이 높다는 단점이 있다. 그러나 이 방법은 전체 데이터셋을 사용하기 때문에 정확한 경사를 찾을 수 있으며, 이로 인해 안정적인 수렴을 보장한다. 그러나 이 방법은 전체 데이터셋이 메모리에 들어갈 수 있어야 하며, 그렇지 않은 경우 다른 경사 하강법 알고리즘을 사용해야 한다.




아이리스 예제
https://wikidocs.net/263123