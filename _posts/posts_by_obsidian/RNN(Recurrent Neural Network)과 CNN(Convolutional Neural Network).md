---
title: \[Category\] Title
excerpt: 
categories: 
tags:
---
# 0. RNN(Recurrent Neural Network)

`RNN(순환 신경망)`은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델이다. 번역기를 생각해보면 입력은 번역하고자 하는 단어의 시퀀스인 문장이다. 출력에 해당되는 번역된 문장 또한 단어의 시퀀스이다. 이와 같이 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 한다. 그 중 RNN은 가장 기본적인 인공 신경망 시퀀스 모델이다.

- `Sequence` : 순서가 있는 데이터(ex. Text -> 문맥이라는 순서가 있음, 시계열 데이터 -> 시간이라는 순서가 있음).

RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 `cell`이라고 한다. 이 cell은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 `메모리 셀` 또는 `RNN 셀`이라고 한다.

시점 t에서 다음 시점인 t+1로 보내는 값을 `hidden state(은닉 상태)`라고 한다. 다시 말해 t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 hidden state 값을 t 시점의 hidden state 계산을 위한 입력값으로 사용한다.

![[Pasted image 20250917144226.png]]

feed forward 신경망에서는 뉴런이라는 단위를 사용했지만, RNN에서는 뉴런이라는 단위 보다는 다른 용어를 사용한다. 입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현을 자주 사용한다. 위의 그림에서 회색과 초록색으로 표현한 각 네모들은 기본적으로 벡터 단위를 가정하고 있다.

![[Pasted image 20250917145111.png]]

![[Pasted image 20250917145254.png]]

RNN은 입력과 출력의 길이를 다르게 설계할 수 있으므로 다양한 용도로 사용할 수 있다.
![[Pasted image 20250917145452.png]]
![[Pasted image 20250917145501.png]]

# 0-1. RNN 수식

![[Pasted image 20250917150104.png]]
![[Pasted image 20250917150230.png]]
![[Pasted image 20250917152550.png]]

h_t를 계산하기 위한 활성화 함수로는 주로 tanh(하이퍼볼릭탄젠트 함수)가 많이 사용된다.


# 1. CNN(Convolutional Neural Network)

CNN(합성곱 신경망)은 이미지 처리에 탁월한 성능을 보이는 신경망이다. CNN은 크게 `Convolution layer(합성곱층)`와 `Pooling layer(풀링층)`으로 구성된다.

![[Pasted image 20250917153521.png]]

위 그림에서 CONV는 합성곱 연산을 의미하고, 합성곱 연산의 결과가 활성화 함수 ReLU를 지난다. 이 두 과정을 `Convolution layer`라고 한다. 그 후에 POOL이라는 구간을 지나는데 이는 풀링 연산을 의미하며, `Pooling layer`라고 한다.

## 1-1. CNN의 대두

![[Pasted image 20250917154219.png]]
위 그림은 알파벳 Y를 정자로 쓴 손글씨와 휘갈겨 쓴 손글씨 2개를 2차원 텐서인 행렬로 표현한 것이다. 기계가 보기에는 각 픽셀마다 가진 값이 대부분 상이하므로 사실상 다른 값을 가진 입력이다.

위 손글씨를 다층 퍼셉트론으로 분류한다고 하면, 이미지를 1차원 텐서인 벡터로 변환하고 다층 퍼셉트론의 입력층으로 사용해야 한다.

![[Pasted image 20250917154404.png]]

1차원으로 변환된 결과는 사람이 보기에도 이게 원래 어떤 이미지였는지 알아보기 어려우며, 이는 기계도 마찬가지다. 위와 같이 결과는 변환 전에 가지고 있던 공간적인 구조(spatial structure) 정보가 유실된 상태이다. 여기서 공간적인 구조 정보라는 것은 거리가 가까운 어떤 픽셀들끼리는 어떤 연관이 있고, 어떤 픽셀들끼리는 값이 비슷하거나 등을 포함하고 있다. 결국 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고, 이를 위해 합성곱 신경망을 사용한다.

## 1-2. Convolution operation(합성곱 연산)

합성곱층은 합성곱 연산을 통해서 이미지의 특징을 추출하는 역할을 한다. `kernel` 또는 `filter`라 불리는 (n x m) 행렬을 이미지의 처음부터 끝까지 겹치며 훑으면서, 각 이미지와 커널의 원소 값을 곱해 모두 더한 값을 출력한다. 커널은 일반적으로 (3x3) 또는 (5x5)를 많이 사용한다.

- `stride`가 1인 합성곱 연산
![[Pasted image 20250917155319.png]]
![[Pasted image 20250917155326.png]]
![[Pasted image 20250917155339.png]]
![[Pasted image 20250917155354.png]]
![[Pasted image 20250917155406.png]]

위 연산을 총 9번의 스텝까지 마치면, 결과(Feature map이라 한다)는 다음과 같다.

![[Pasted image 20250917155450.png]]
- `stride`가 2인 합성곱 연산
![[Pasted image 20250917155520.png]]

### 패딩

위에서 stride = 1인 합성곱 연산을 했을 때, 크기가 줄어드는 것을 확인했다. 입력과 특성맵의 크기를 보존하고 싶다면, 패딩을 해주면 된다.
![[Pasted image 20250917160029.png]]


## 1-3. 가중치와 편향

![[Pasted image 20250917160440.png]]
합성곱에서 이미지 전체를 훑으면서 사용되는 가중치는 `w0`, `w1`, `w2`, `w3` 4개 뿐이다. 만약 3x3 이미지를 1차원 벡터로 만들어서 MLP에 넣었다면 `9x4=36`개의 가중치가 필요하다는 것을 생각해보자(`4` : 위 그림에서 hidden layer의 node 개수). **즉, 합성곱 신경망은 다층 퍼셉트론을 사용할 때보다 훨씬 적은 수의 가중치를 사용하며 공간적 구조 정보를 보존한다는 특징이 있다.**

합성곱에도 편향을 적용할 수 있다.
![[Pasted image 20250917160858.png]]

## 1-4. 다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)

만약, 다수의 채널을 가진 입력 데이터를 가지고 합성곱 연산을 한다고 하면 커널의 채널 수도 입력의 채널 수만큼 존재해야 한다. 다시 말해, 입력 데이터의 채널 수와 커널의 채널 수는 같아야 한다. 채널 수가 같으므로 합성곱 연산을 채널마다 수행한다. 그리고 그 결과를 모두 더하여 최종 특성 맵을 얻는다.

![[Pasted image 20250917161318.png]]

## 1-5. Pooling(풀링)

일반적으로 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적이다. 풀링 층에서는 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어진다. 풀링 연산에는 일반적으로 `최대 풀링(max pooling)`, `평균 풀링(average pooling)`이 사용된다.

- `최대 풀링(max pooling)`
![[Pasted image 20250917161640.png]]

- `평균 풀링(average pooling)`
평균 풀링은 평균값을 추출하는 연산을 수행한다.

풀링 연산은 `kernel`과 `stride` 개념이 존재한다는 점에서 합성곱 연산과 유사하지만, 학습해야 할 가중치가 없으며 연산 후에 채널 수가 변하지 않는다는 차이가 있다.